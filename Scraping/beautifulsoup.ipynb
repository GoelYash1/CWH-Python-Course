{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5d41ba8-628e-4f9b-8ac1-3340f78c7ac6",
   "metadata": {},
   "source": [
    "# Using Beautiful Soup for Data Collection\n",
    "\n",
    "---\n",
    "\n",
    "## 1. What is BeautifulSoup?\n",
    "\n",
    "**BeautifulSoup** is a Python library used to parse HTML and XML documents.  \n",
    "It creates a *parse tree* from page content, making it easy to extract data.  \n",
    "It is often used with `requests` to scrape websites.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Installing BeautifulSoup\n",
    "\n",
    "Install both **beautifulsoup4** and a parser like **lxml**:\n",
    "\n",
    "```bash\n",
    "pip install beautifulsoup4 lxml\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Creating a BeautifulSoup Object\n",
    "\n",
    "**Example:**\n",
    "\n",
    "```python\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    " \n",
    "url = \"https://example.com\"\n",
    "response = requests.get(url)\n",
    " \n",
    "soup = BeautifulSoup(response.text, \"lxml\")\n",
    "```\n",
    "\n",
    "**Notes:**\n",
    "- `response.text`: HTML content.  \n",
    "- `\"lxml\"`: A fast and powerful parser (you can also use `\"html.parser\"`).\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Understanding the HTML Structure\n",
    "\n",
    "BeautifulSoup treats the page like a tree.  \n",
    "You can search and navigate through **tags**, **classes**, **ids**, and **attributes**.\n",
    "\n",
    "**Example HTML:**\n",
    "\n",
    "```html\n",
    "<html>\n",
    "  <body>\n",
    "    <h1>Title</h1>\n",
    "    <p class=\"description\">This is a paragraph.</p>\n",
    "    <a href=\"/page\">Read more</a>\n",
    "  </body>\n",
    "</html>\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Common Methods in BeautifulSoup\n",
    "\n",
    "### 5.1 Accessing Elements\n",
    "\n",
    "Access the first occurrence of a tag:\n",
    "\n",
    "```python\n",
    "soup.h1\n",
    "```\n",
    "\n",
    "Get the text inside a tag:\n",
    "\n",
    "```python\n",
    "soup.h1.text\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 5.2 `find()` Method\n",
    "\n",
    "Finds the first matching element:\n",
    "\n",
    "```python\n",
    "soup.find(\"p\")\n",
    "```\n",
    "\n",
    "Find a tag with specific attributes:\n",
    "\n",
    "```python\n",
    "soup.find(\"p\", class_=\"description\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 5.3 `find_all()` Method\n",
    "\n",
    "Finds all matching elements:\n",
    "\n",
    "```python\n",
    "soup.find_all(\"a\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 5.4 Using `select()` and `select_one()`\n",
    "\n",
    "Select elements using **CSS selectors**:\n",
    "\n",
    "```python\n",
    "soup.select_one(\"p.description\")\n",
    "```\n",
    "\n",
    "```python\n",
    "soup.select(\"a\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Extracting Attributes\n",
    "\n",
    "Get the value of an attribute, such as `href` from an `<a>` tag:\n",
    "\n",
    "```python\n",
    "link = soup.find(\"a\")\n",
    "print(link[\"href\"])\n",
    "```\n",
    "\n",
    "Or using `.get()`:\n",
    "\n",
    "```python\n",
    "print(link.get(\"href\"))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Traversing the Tree\n",
    "\n",
    "Access **parent elements**:\n",
    "\n",
    "```python\n",
    "soup.p.parent\n",
    "```\n",
    "\n",
    "Access **children elements**:\n",
    "\n",
    "```python\n",
    "list(soup.body.children)\n",
    "```\n",
    "\n",
    "Find the **next sibling**:\n",
    "\n",
    "```python\n",
    "soup.h1.find_next_sibling()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 8. Handling Missing Elements Safely\n",
    "\n",
    "Always check if an element exists before accessing it:\n",
    "\n",
    "```python\n",
    "title_tag = soup.find(\"h1\")\n",
    "if title_tag:\n",
    "    print(title_tag.text)\n",
    "else:\n",
    "    print(\"Title not found\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 9. Summary\n",
    "\n",
    "- **BeautifulSoup** helps parse and navigate HTML easily.  \n",
    "- Use `.find()`, `.find_all()`, `.select()`, and `.select_one()` to locate data.  \n",
    "- Always inspect the website's structure before writing scraping logic.  \n",
    "- Combine **BeautifulSoup** with `requests` for full scraping workflows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99da0d9-e608-4607-b819-3506c816f97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15c7fa5-2eb0-46d1-8c4d-c5f974d24cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698c8601-847e-4604-9e2b-4440271ca39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"requests_ran.flag\"):\n",
    "    print(\"requests first...\")\n",
    "    subprocess.run([\"jupyter\", \"nbconvert\", \"--to\", \"notebook\", \"--execute\", \"requests.ipynb\", \"--output\", \"requests.ipynb\"])\n",
    "else:\n",
    "    print(\"requests already ran. Continuing...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7fbf0a-2825-488f-b410-778be624202f",
   "metadata": {},
   "outputs": [],
   "source": [
    "html_dir = os.path.join(os.getcwd(), \"htmls\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34114ea-c2d6-4168-8dcf-e4137a925225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to store (page_number, content)\n",
    "html_contents = []\n",
    "\n",
    "# Loop over all files in htmls directory\n",
    "for file in os.listdir(html_dir):\n",
    "    file_path = os.path.join(html_dir, file)\n",
    "\n",
    "    # Check if it's a .html file\n",
    "    if os.path.isfile(file_path) and file.endswith(\".html\"):\n",
    "        # Extract the page number from filename, e.g., page23.html -> 23\n",
    "        match = re.search(r'page(\\d+)\\.html', file)\n",
    "        if match:\n",
    "            page_number = int(match.group(1))\n",
    "            # Read file content\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                content = f.read()\n",
    "            html_contents.append((page_number, content))\n",
    "\n",
    "# Sort by page number\n",
    "html_contents.sort(key=lambda x: x[0])\n",
    "\n",
    "# Extract only the content in order\n",
    "html_list = [content for _, content in html_contents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eeda871-3b3c-4651-be7b-bcf4b4e1e141",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "articles_list = []\n",
    "for content in html_list:\n",
    "    soup = BeautifulSoup(content, \"html.parser\")\n",
    "    articles = soup.select(\"article.product_pod\")\n",
    "    articles_list.append(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ccfa996-39d2-443a-ad09-e3d8a40cc8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "items = []\n",
    "mp = {\n",
    "    \"One\":1,\n",
    "    \"Two\":2,\n",
    "    \"Three\":3,\n",
    "    \"Four\":4,\n",
    "    \"Five\":5,\n",
    "}\n",
    "for articles in articles_list:\n",
    "    for article in articles:\n",
    "        title = article.find(\"h3\").find(\"a\")[\"title\"]\n",
    "        price = float(article.select_one(\"p.price_color\").text.split(\"Â£\")[1])\n",
    "        rating_element = article.select_one(\"p.star-rating\")\n",
    "        rating =  mp[rating_element[\"class\"][1]]\n",
    "        items.append([title, price, rating])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1468f16-706a-4574-81d1-1a85b7df525a",
   "metadata": {},
   "outputs": [],
   "source": [
    "items[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28384e82-eff6-492f-9ff0-af4b486f126c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(items, columns = [\"Book Title\",\"Price\",\"Rating\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4c0eff-f00b-43b3-8863-a8adaf8fc838",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5c0b62-b938-480d-925b-2f1adc8dedad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"data.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
